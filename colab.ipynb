{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution in Google Colab\n",
    "Welcome to this Jupyter notebook! The primary goal of this notebook is to facilitate the calculation of delay between the visual occurrence of an explosion and its corresponding sound in a video. By understanding this delay, we can estimate the distance between the explosion's location and the camera's position, aiding in geolocation tasks.\n",
    "\n",
    "Advantages of using this notebook:\n",
    "\n",
    "Simplicity: No need for a local Python setup. Run everything directly in Google Colab.\n",
    "Accessibility: Easily calculate distance based on sound and visual delays without diving deep into the codebase.\n",
    "\n",
    "http://colab.research.google.com/github/davidnewschool/sound-delay/blob/develop/colab.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup\n",
    "To ensure we're using the most updated and accurate methods for our analysis, we'll clone a specific repository that contains the latest code for distance calculation. Cloning ensures you're working with the most recent and optimized version of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone -b develop https://github.com/davidnewschool/sound-delay.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.getcwd().endswith('/sound-delay'):\n",
    "    %cd sound-delay\n",
    "\n",
    "# Show the files\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r colab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Calculation\n",
    "The central premise here is the difference in speed between light and sound. When an explosion occurs, we typically see the explosion (light travels faster) before we hear it (sound travels slower). By analyzing this delay, we can make an educated guess about how far the camera was from the explosion, helping in geolocation efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the Delay in a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the # on the next line if you run it local and want the mathplotlib window as a popup to interact with. Does not work on Google Colab.\n",
    "# %matplotlib tk\n",
    "\n",
    "# The script requires a video file to process. You can specify the video file in two ways:\n",
    "# Directly within the script (as shown below):\n",
    "# %run plot_delay.py example/video.mp4\n",
    "\n",
    "#Â If you don't specify here, don't worry! The script will prompt you for the file path later.\n",
    "%run plot_delay.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Analysis\n",
    "In this segment, we venture into experimental territories. We're attempting to automate two significant aspects:\n",
    "\n",
    "1. Visual Detection: Identifying the exact moment the explosion is seen in the video.\n",
    "2. Auditory Detection: Pinpointing when the explosion sound is captured in the audio.\n",
    "\n",
    "This experimental approach applies certain mathematical concepts to achieve automation. However, it's essential to approach the results with caution. The current methods might not be entirely reliable, especially with videos that are handheld or zoomed during recording."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mathematical Concepts Used in Automation and Audio Analysis\n",
    "\n",
    "The notebook employs specific mathematical techniques to automate the identification of visual and auditory cues in videos:\n",
    "\n",
    "##### 1. Rate of Change (Derivative) for Visual Cues\n",
    "\n",
    "- The visual detection primarily relies on the rate of change in the data. This is achieved by calculating the derivative of the data concerning time.\n",
    "- The derivative helps in determining how data values change over a small interval. In the context of video data, a significant spike in the derivative might indicate a sudden change in visual intensity, such as the occurrence of an explosion.\n",
    "- Refer to the `compute_derivative` function for the implementation of this concept, where the rate of change of data values over time is computed.\n",
    "\n",
    "##### 2. Standard Deviation for Loudness Spikes Detection in Auditory Cues\n",
    "\n",
    "- The notebook uses the concept of standard deviation to detect significant spikes in loudness. Standard deviation measures the dispersion or variability of a set of values. In this context, values with a high deviation from the mean could indicate significant events like the sound of an explosion.\n",
    "  \n",
    "- Specifically, a threshold is determined based on the standard deviation of the loudness derivative:\n",
    "    ```python\n",
    "    threshold = 3 * np.std(loudness_derivative)\n",
    "    ```\n",
    "  This threshold helps detect rapid changes in sound intensity. A value in the loudness derivative that surpasses this threshold is considered a significant spike, potentially indicating the auditory cue of the explosion.\n",
    "\n",
    "##### 3. Derivative Calculation for Rapid Changes Detection in Auditory Cues\n",
    "\n",
    "- The derivative is also employed in the audio analysis to identify rapid changes in the data. The function `compute_derivative` calculates the derivative of the loudness concerning time, helping detect moments where the loudness changes abruptly.\n",
    "    ```python\n",
    "    loudness_derivative, _ = compute_derivative(adjusted_loudness, adjusted_time_audio)\n",
    "    ```\n",
    "  In the context of audio analysis, a significant spike in the derivative might indicate a sudden increase in volume, such as the sound of an explosion.\n",
    "\n",
    "By using these mathematical techniques, the notebook can automatically identify the visual occurrence of an explosion and its corresponding sound, facilitating the calculation of the delay between them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental Visual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get min and max of time axis\n",
    "time_min = time_audio[0]\n",
    "time_max = time_audio[-1]\n",
    "\n",
    "# Get min and max of\n",
    "amp_min = np.min(red_intensity) - 0.1*( np.max(red_intensity) - np.min(red_intensity) )\n",
    "amp_max = np.max(red_intensity) + 0.1*( np.max(red_intensity) - np.min(red_intensity) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_highlight_rectangle(fig, x0, x1, fillcolor=\"grey\", opacity=0.3, layer=\"below\", line_width=0):\n",
    "    \"\"\"\n",
    "    Add a rectangular shape to a plotly figure to highlight a region.\n",
    "\n",
    "    Parameters:\n",
    "    - fig (plotly.graph_objs._figure.Figure): The figure to add the rectangle to.\n",
    "    - x0 (float): The starting x-coordinate of the rectangle.\n",
    "    - x1 (float): The ending x-coordinate of the rectangle.\n",
    "    - fillcolor (str): The color of the rectangle. Default is \"grey\".\n",
    "    - opacity (float): The opacity of the rectangle. Default is 0.3.\n",
    "    - layer (str): Whether to place the rectangle below or above the traces. Default is \"below\".\n",
    "    - line_width (int): The width of the rectangle's line. Default is 0.\n",
    "    \"\"\"\n",
    "    \n",
    "    fig.add_shape(\n",
    "        type=\"rect\",\n",
    "        xref=\"x\",\n",
    "        yref=\"paper\",  # relative to the entire height of the plot\n",
    "        x0=x0,\n",
    "        x1=x1,\n",
    "        y0=0,\n",
    "        y1=1,\n",
    "        fillcolor=fillcolor,\n",
    "        opacity=opacity,\n",
    "        layer=layer,\n",
    "        line_width=line_width,\n",
    "    )\n",
    "\n",
    "# Example usage:\n",
    "# add_highlight_rectangle(fig, slowdown_time, first_spike_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_frame_ticks(fig, time_audio, frame_rate):\n",
    "    \"\"\"\n",
    "    Configures the x-axis of a plotly figure to have minor gridlines for every frame and major gridlines for every second.\n",
    "\n",
    "    Parameters:\n",
    "    - fig (plotly.graph_objs._figure.Figure): The figure to update.\n",
    "    - time_audio (list or array-like): A list or array containing time points for the audio. The last item is assumed to represent the total duration.\n",
    "    - frame_rate (float): Frame rate of the audio.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Update the layout\n",
    "    fig.update_layout(\n",
    "        xaxis=dict(\n",
    "            ticklen=10,  # Length of major ticks\n",
    "            showgrid=True,  # Gridlines\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Calculate total duration in seconds (round up)\n",
    "    total_seconds = int(np.ceil(time_audio[-1]))\n",
    "\n",
    "    # Update x-axis for minor ticks representing each frame within a second when zoomed in\n",
    "    fig.update_xaxes(\n",
    "        minor_tickmode=\"linear\",\n",
    "        minor_tick0=0,\n",
    "        minor_dtick=1/frame_rate,\n",
    "        minor_ticklen=0,  # Length of minor ticks\n",
    "        minor_showgrid=True,\n",
    "        minor_nticks=int(frame_rate * total_seconds)  # Maximum number of minor ticks\n",
    "    )\n",
    "\n",
    "# Example usage:\n",
    "# configure_frame_ticks(fig, time_audio_list, frame_rate_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_derivative(data, time):\n",
    "    \"\"\"Compute the derivative and return the adjusted time.\"\"\"\n",
    "    derivative = np.diff(data) / np.diff(time)\n",
    "    adjusted_time = time[:-1]\n",
    "    return derivative, adjusted_time\n",
    "\n",
    "def normalize_data(data):\n",
    "    \"\"\"Normalize data between 0 and 1 based on its range.\"\"\"\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add Loudness and Red Intensity traces\n",
    "fig.add_trace(go.Scatter(x=time_audio, y=loudness, mode='lines', name='loudness', line=dict(color='blue'), yaxis='y2'))\n",
    "fig.add_trace(go.Scatter(x=time_video, y=red_intensity, mode='lines', name='red intensity', line=dict(color='red')))\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(\n",
    "    xaxis=dict(title=\"Time [seconds]\", range=[time_min, time_max], rangeslider=dict(visible=True), type='linear'),\n",
    "    yaxis_title=\"Red Intensity\",\n",
    "    yaxis_range=[amp_min, amp_max],\n",
    "    yaxis2=dict(title=\"Loudness\", overlaying='y', side='right'),\n",
    "    showlegend=True,\n",
    "    title=\"Comparison of Loudness and Red Intensity over Time\"\n",
    ")\n",
    "\n",
    "configure_frame_ticks(fig, time_audio, frame_rate)\n",
    "\n",
    "# Save the plot to the same path/name as the input video\n",
    "# output_image_path = video_path[:-3] + 'png'\n",
    "# fig.write_image(output_image_path)  # Requires plotly to be installed with the \"orca\" extra: pip install plotly[orca]\n",
    "\n",
    "# Display the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- BOOM ----\n",
    "\n",
    "# Derive red_intensity to detect rapid changes which may correspond to visual anomalies (e.g., flash from an explosion).\n",
    "red_intensity_derivative, time_video_deriv = compute_derivative(red_intensity, time_video)\n",
    "\n",
    "# Identify the spike in the derivative, which likely indicates the start of the explosion.\n",
    "first_spike_deriv_red_index = np.argmax(red_intensity_derivative)\n",
    "\n",
    "# Detect the point where the increase in intensity begins to slow down post-explosion.\n",
    "first_decreasing_point_after_spike = np.argmax(red_intensity_derivative[first_spike_deriv_red_index:] < red_intensity_derivative[first_spike_deriv_red_index])\n",
    "slowdown_time = time_video_deriv[first_spike_deriv_red_index + first_decreasing_point_after_spike]\n",
    "\n",
    "# ---- SOUND ----\n",
    "\n",
    "# Adjust the audio data to focus on the timeframe after the visual anomaly was detected.\n",
    "start_idx_audio = np.where(time_audio >= slowdown_time)[0][0]\n",
    "adjusted_time_audio, adjusted_loudness = time_audio[start_idx_audio:], loudness[start_idx_audio:]\n",
    "\n",
    "# Derive the loudness to detect rapid changes in sound intensity.\n",
    "loudness_derivative, _ = compute_derivative(adjusted_loudness, adjusted_time_audio)\n",
    "\n",
    "# Define a threshold to detect the significant spike in loudness which likely corresponds to the sound from the explosion.\n",
    "threshold = 3 * np.std(loudness_derivative)\n",
    "first_spike_index = np.argmax(loudness_derivative > threshold)\n",
    "first_spike_time, first_spike_value = adjusted_time_audio[first_spike_index], adjusted_loudness[first_spike_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- PLOTTING AND ANNOTATING ----\n",
    "\n",
    "# Create a figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add Red Intensity and Loudness traces\n",
    "fig.add_trace(go.Scatter(x=time_video, y=red_intensity, mode='lines', name='Red Intensity', line=dict(color='red')))\n",
    "fig.add_trace(go.Scatter(x=time_audio, y=loudness, mode='lines', name='Loudness', line=dict(color='blue'), yaxis='y2'))\n",
    "\n",
    "# Add annotations\n",
    "fig.add_annotation(x=slowdown_time, y=red_intensity[np.where(time_video == slowdown_time)[0][0]], \n",
    "                   text=f'Derivative starts decreasing at {slowdown_time:.2f} s', showarrow=True, arrowhead=2, yref=\"y\")\n",
    "fig.add_annotation(x=first_spike_time, y=first_spike_value, \n",
    "                   text=f'First Loudness Spike at {first_spike_time:.2f} s', showarrow=True, arrowhead=2, yref=\"y2\")\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(\n",
    "    xaxis=dict(title=\"Time [seconds]\", rangeslider=dict(visible=True), type='linear'),\n",
    "    yaxis_title=\"Red Intensity\",\n",
    "    yaxis_range=[amp_min, amp_max],\n",
    "    yaxis2=dict(title=\"Loudness\", overlaying='y', side='right'),\n",
    "    showlegend=True,\n",
    "    title=\"Comparison of Loudness and Red Intensity over Time\"\n",
    ")\n",
    "\n",
    "add_highlight_rectangle(fig, slowdown_time, first_spike_time)\n",
    "configure_frame_ticks(fig, time_audio, frame_rate)\n",
    "\n",
    "# Display the visual analysis.\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental: Further Sound Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "from scipy.signal import spectrogram\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the audio signal from the audio_path\n",
    "fs, sig = wavfile.read(audio_path)\n",
    "\n",
    "# Check if the audio signal has multiple channels (e.g., stereo). If so, use only one channel for simplicity.\n",
    "if len(sig.shape) > 1:\n",
    "    sig = sig[:, 0]\n",
    "\n",
    "# Compute the spectrogram using scipy\n",
    "frequencies, times, Sxx = spectrogram(sig, fs=fs, nperseg=4096, noverlap=2048, scaling='density')\n",
    "\n",
    "# Convert amplitude to dB\n",
    "Sxx_db = 10 * np.log10(Sxx)\n",
    "\n",
    "# Plot the original signals and the spectrogram\n",
    "fig_spec = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=('Original Signals', 'Spectrogram'))\n",
    "\n",
    "# Add traces for the original signals\n",
    "fig_spec.add_trace(go.Scatter(x=time_audio, y=loudness, mode='lines', name='Loudness', line=dict(color='blue')), row=1, col=1)\n",
    "\n",
    "# Add the spectrogram heatmap\n",
    "fig_spec.add_trace(go.Heatmap(x=times, y=frequencies, z=Sxx_db, colorscale='Viridis'), row=2, col=1)\n",
    "\n",
    "# Update layout\n",
    "fig_spec.update_layout(title=\"Original Signals and Spectrogram\")\n",
    "fig_spec.update_yaxes(title_text=\"Frequency (Hz)\", row=2, col=1)\n",
    "fig_spec.update_xaxes(title_text=\"Time (s)\",rangeslider=dict(visible=True), row=2, col=1)\n",
    "\n",
    "fig_spec.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for frequencies below the 'frequency' threshold (e.g., 200 Hz, 300 Hz) - where you would only expect sounds like explosions.\n",
    "# Note: Adjust the 'frequency' variable as needed.\n",
    "frequency = 200\n",
    "mask = frequencies < frequency\n",
    "low_freq_amplitudes = Sxx[mask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average amplitude for frequencies below the 'frequency' threshold for each time step.\n",
    "avg_low_freq_amplitude = np.mean(low_freq_amplitudes, axis=0)\n",
    "\n",
    "# Plot the original signals, spectrogram, and the low-frequency amplitude\n",
    "fig_combined = make_subplots(rows=3, cols=1, shared_xaxes=True, \n",
    "                             subplot_titles=('Original Signals', 'Spectrogram', f'Amplitude (<{frequency} Hz)'))\n",
    "\n",
    "# Add traces for the original signals\n",
    "fig_combined.add_trace(go.Scatter(x=time_audio, y=loudness, mode='lines', name='Loudness', line=dict(color='blue')), row=1, col=1)\n",
    "fig_combined.add_trace(go.Scatter(x=times, y=avg_low_freq_amplitude, mode='lines', name=f'Amplitude (<{frequency} Hz)', line=dict(color='green')), row=3, col=1)\n",
    "\n",
    "# Add the spectrogram heatmap\n",
    "fig_combined.add_trace(go.Heatmap(x=times, y=frequencies, z=Sxx_db, colorscale='Viridis', showscale=False), row=2, col=1)\n",
    "\n",
    "# Update layout\n",
    "fig_combined.update_layout(title=\"Original Signals, Spectrogram, and Low-Frequency Amplitude\")\n",
    "fig_combined.update_yaxes(title_text=\"Frequency (Hz)\", row=2, col=1)\n",
    "fig_combined.update_xaxes(title_text=\"Time (s)\", row=3, col=1)\n",
    "\n",
    "fig_combined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the avg_low_freq_amplitude to match loudness amplitude range\n",
    "avg_low_freq_amplitude_norm = (avg_low_freq_amplitude - np.min(avg_low_freq_amplitude)) / (np.max(avg_low_freq_amplitude) - np.min(avg_low_freq_amplitude))\n",
    "avg_low_freq_amplitude_norm = avg_low_freq_amplitude_norm * (np.max(loudness) - np.min(loudness)) + np.min(loudness)\n",
    "\n",
    "# Plot the original signals, spectrogram, and the normalized low-frequency amplitude\n",
    "fig_combined = make_subplots(rows=3, cols=1, shared_xaxes=True, \n",
    "                             subplot_titles=('Original Signals', 'Spectrogram', f'Amplitude (<{frequency} Hz)'))\n",
    "\n",
    "# Add traces for the original signals\n",
    "fig_combined.add_trace(go.Scatter(x=time_audio, y=loudness, mode='lines', name='Loudness', line=dict(color='blue')), row=1, col=1)\n",
    "fig_combined.add_trace(go.Scatter(x=times, y=avg_low_freq_amplitude_norm, mode='lines', name=f'Normalized Amplitude (<{frequency} Hz)', line=dict(color='green')), row=3, col=1)\n",
    "\n",
    "# Add the spectrogram heatmap\n",
    "fig_combined.add_trace(go.Heatmap(x=times, y=frequencies, z=Sxx_db, colorscale='Viridis', showscale=False), row=2, col=1)\n",
    "\n",
    "# Update layout\n",
    "fig_combined.update_layout(title=\"Original Signals, Spectrogram, and Normalized Low-Frequency Amplitude\")\n",
    "fig_combined.update_yaxes(title_text=\"Frequency (Hz)\", row=2, col=1)\n",
    "fig_combined.update_xaxes(title_text=\"Time (s)\", row=3, col=1)\n",
    "\n",
    "fig_combined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- BOOM ----\n",
    "\n",
    "# Derive red_intensity to detect rapid changes which may correspond to visual anomalies (e.g., flash from an explosion).\n",
    "red_intensity_derivative, time_video_deriv = compute_derivative(red_intensity, time_video)\n",
    "\n",
    "# Normalize the derivative so it's in the same amplitude range as the original signals\n",
    "red_intensity_derivative_norm = (red_intensity_derivative - np.min(red_intensity_derivative)) / (np.max(red_intensity_derivative) - np.min(red_intensity_derivative))\n",
    "red_intensity_derivative_norm = red_intensity_derivative_norm * (np.max(red_intensity) - np.min(red_intensity)) + np.min(red_intensity)\n",
    "\n",
    "# Identify the spike in the derivative, which likely indicates the start of the explosion.\n",
    "first_spike_deriv_red_index = np.argmax(red_intensity_derivative)\n",
    "\n",
    "# Detect the point where the increase in intensity begins to slow down post-explosion.\n",
    "first_decreasing_point_after_spike = np.argmax(red_intensity_derivative[first_spike_deriv_red_index:] < red_intensity_derivative[first_spike_deriv_red_index])\n",
    "slowdown_time = time_video_deriv[first_spike_deriv_red_index + first_decreasing_point_after_spike]\n",
    "\n",
    "# ---- SOUND ----\n",
    "\n",
    "loudness_low = avg_low_freq_amplitude_norm\n",
    "start_idx_audio = np.where(times >= slowdown_time)[0][0]\n",
    "adjusted_time_audio, adjusted_loudness_low = times[start_idx_audio:], loudness_low[start_idx_audio:]\n",
    "\n",
    "# Compute derivative and threshold for loudness\n",
    "loudness_low_derivative = np.diff(adjusted_loudness_low) / np.diff(adjusted_time_audio)\n",
    "threshold = 3 * np.std(loudness_low_derivative)\n",
    "\n",
    "# Detect first significant spike\n",
    "first_spike_index = np.argmax(loudness_low_derivative > threshold)\n",
    "first_spike_time, first_spike_value = adjusted_time_audio[first_spike_index], adjusted_loudness_low[first_spike_index]\n",
    "\n",
    "# Calculate standard deviation\n",
    "std_dev_loudness_low = np.std(adjusted_loudness_low[:first_spike_index])\n",
    "\n",
    "# ---- PLOTTING AND ANNOTATING ----\n",
    "\n",
    "fig_all = go.Figure()\n",
    "\n",
    "# Plot signals\n",
    "fig_all.add_trace(go.Scatter(x=times, y=loudness_low, mode='lines', name=f'Amplitude (<{frequency} Hz)', line=dict(color='green'), yaxis='y2'))\n",
    "fig_all.add_trace(go.Scatter(x=time_video, y=red_intensity, mode='lines', name='Red Intensity', line=dict(color='red')))\n",
    "fig_all.add_trace(go.Scatter(x=time_audio, y=loudness, mode='lines', name='Loudness', line=dict(color='blue'), yaxis='y2'))\n",
    "\n",
    "# Annotations\n",
    "derivative_y = red_intensity_derivative_norm[first_spike_deriv_red_index + first_decreasing_point_after_spike]\n",
    "fig_all.add_annotation(x=slowdown_time, y=derivative_y, text=f'Derivative starts decreasing at {slowdown_time:.2f} s', showarrow=True, arrowhead=2, yref=\"y\", bgcolor=\"rgba(255,255,255,0.7)\")\n",
    "fig_all.add_annotation(x=first_spike_time, y=first_spike_value, text=f'First Amplitude Spike at {first_spike_time:.2f} s', showarrow=True, arrowhead=2, yref=\"y2\", bgcolor=\"rgba(255,255,255,0.7)\")\n",
    "\n",
    "# Layout\n",
    "fig_all.update_layout(\n",
    "    title=\"Comparison of Loudness, Amplitude below Frequency, and Red Intensity Over Time\",\n",
    "    xaxis=dict(title=\"Time (s)\", rangeslider=dict(visible=True)),\n",
    "    yaxis=dict(title=\"Red Intensity\"),\n",
    "    yaxis2=dict(title=\"Sound\", overlaying='y', side='right'),\n",
    "    legend=dict(x=0.5, y=-0.7, xanchor='center', orientation='h')\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "fig_all.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_highlight_rectangle(fig_all, slowdown_time, first_spike_time)\n",
    "configure_frame_ticks(fig_all, time_audio, frame_rate)\n",
    "\n",
    "# Display the updated plot\n",
    "fig_all.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slowdown_time, first_spike_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run distance.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Helper for handling video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a mp4 file from a URL \n",
    "You can enter any URL (that links directly to a mp4 file) to download into the file drive of this running colab. Files will not be stored longterm and be deleted after you stop the runtime (or timeout)\n",
    "\n",
    "Use Online Services like TwitterVideoDownloader.com or ttvdl.com (TikTok) to get a .mp4 link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Ask user for the URL\n",
    "url = input(\"Please enter the URL of the MP4 file: \")\n",
    "\n",
    "# Define a suitable filename based on the URL\n",
    "filename = url.split('/')[-1]  # This will take the last part of the URL as the filename. \n",
    "\n",
    "response = requests.get(url)\n",
    "with open(filename, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Display a success message in the notebook\n",
    "display(HTML(f\"<span style='color: green;'>File downloaded successfully as <b>{filename}</b></span>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trimming video\n",
    "This Python script enables you to trim a video by specifying the start and end times, creating a new video clip containing only the desired segment. You can use this script to extract specific portions of a video for further editing or sharing.\n",
    "\n",
    "How to Use\n",
    "Input Video File: The script will prompt you to enter the path of the MP4 video file you want to trim. Please provide the full file path, including the file extension (e.g: example/video.mp4).\n",
    "\n",
    "Video Duration: After loading the video, the script will display the total duration of the video in seconds. This information helps you determine the range for trimming.\n",
    "\n",
    "Specify Trimming Times: Enter the start and end times (in seconds) for the portion of the video you want to keep. The script will cut the video from the specified start time to the specified end time.\n",
    "\n",
    "Output Video File: The trimmed video will be saved with a \"-cut\" suffix appended to the original filename. For example, if the original file was named \"video.mp4,\" the trimmed video will be named \"video-cut.mp4.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "\n",
    "# Ask the user for the path of the MP4 file\n",
    "video_path = input(\"Please enter the path of the MP4 file: \")\n",
    "\n",
    "# Load the video clip\n",
    "video_clip = VideoFileClip(video_path)\n",
    "\n",
    "# Give the user information about the duration of the clip\n",
    "video_duration = video_clip.duration\n",
    "print(f\"The video duration is {video_duration:.2f} seconds.\")\n",
    "\n",
    "# Define the start and end times for trimming (in seconds)\n",
    "start_input = input(\"Please enter the start time where you want to cut (default is 0): \")\n",
    "start_time = float(start_input) if start_input else 0  # Start time of the trimmed portion\n",
    "\n",
    "end_input = input(f\"Please enter the end time where you want to cut (default is video duration, {video_duration:.2f}): \")\n",
    "end_time = float(end_input) if end_input else video_duration  # End time of the trimmed portion\n",
    "\n",
    "# Trim the video clip\n",
    "trimmed_clip = video_clip.subclip(start_time, end_time)\n",
    "\n",
    "# Generate the output path with a \"-cut\" suffix\n",
    "output_path = video_path.replace(\".mp4\", f\"-cut_{start_time}_{end_time}.mp4\")\n",
    "\n",
    "# Save the trimmed video with audio\n",
    "trimmed_clip.write_videofile(output_path, codec=\"libx264\")\n",
    "\n",
    "# Close the original video clip\n",
    "video_clip.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Cropping\n",
    "This Python script allows you to crop a video by specifying the percentage of the frame to cut from the top, bottom, left, and right sides. You can use this script to customize the framing of a video, removing unwanted portions to focus on specific content.\n",
    "\n",
    "How to Use\n",
    "Input Video File: The script will prompt you to enter the path of the MP4 video file you want to edit. Provide the full file path, including the file extension (e.g: example/video.mp4).\n",
    "\n",
    "Percentage to Cut: You will be asked to specify the percentage (0-100) of each side (top, bottom, left, and right) that you want to cut. Higher percentages will result in more cropping, while lower percentages will retain more of the original frame.\n",
    "\n",
    "Output Video File: The edited video will be saved with the specified cropping percentages appended to the filename. For example, if you entered 10% for top, 5% for bottom, 15% for left, and 20% for right, the output file would be named something like original-video-edit_10_5_15_20.mp4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "import moviepy.video.fx.all as vfx\n",
    "\n",
    "# Input video file path\n",
    "input_file_path = input(\"Please enter the path of the MP4 file: \")\n",
    "\n",
    "# Load the video clip\n",
    "video_clip = VideoFileClip(input_file_path)\n",
    "\n",
    "# Get the dimensions of the video frame\n",
    "frame_width, frame_height = video_clip.size\n",
    "\n",
    "# Ask the user for the percentage to cut from each side\n",
    "top_percentage = float(input(\"Enter the percentage to cut from the top (0-100): \"))\n",
    "bottom_percentage = float(input(\"Enter the percentage to cut from the bottom (0-100): \"))\n",
    "left_percentage = float(input(\"Enter the percentage to cut from the left (0-100): \"))\n",
    "right_percentage = float(input(\"Enter the percentage to cut from the right (0-100): \"))\n",
    "\n",
    "# Output video file path with percentages\n",
    "output_file_path = input_file_path.replace(\".mp4\", f\"-edit_{top_percentage}_{bottom_percentage}_{left_percentage}_{right_percentage}.mp4\")\n",
    "\n",
    "# Calculate the pixel values to cut\n",
    "top_cut = int(frame_height * (top_percentage / 100))\n",
    "bottom_cut = int(frame_height * (bottom_percentage / 100))\n",
    "left_cut = int(frame_width * (left_percentage / 100))\n",
    "right_cut = int(frame_width * (right_percentage / 100))\n",
    "\n",
    "# Crop the video clip\n",
    "cropped_clip = video_clip.crop(y1=top_cut, y2=frame_height - bottom_cut, x1=left_cut, x2=frame_width - right_cut)\n",
    "\n",
    "# Write the edited video to the output file\n",
    "cropped_clip.write_videofile(output_file_path, codec=\"libx264\")\n",
    "\n",
    "print(\"Video editing complete. Saved as\", output_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sound-delay",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
